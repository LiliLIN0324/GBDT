{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a7b95312",
      "metadata": {
        "id": "a7b95312",
        "outputId": "fde493fc-fb93-4f4c-bc35-c66839557c19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9394  898 2398 5906 2343 8225 5506 6451 2670 3497 1087 1819 2308 6084\n",
            " 3724 3184 6387 3728 2702 7883]\n",
            "19\n",
            "7883\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# 用20个 random seeds\n",
        "np.random.seed(0)  # 固定种子以便复现\n",
        "random_seeds = np.random.choice(10000, size=20, replace=False)\n",
        "print(random_seeds)\n",
        "n = 19\n",
        "print(n)\n",
        "print(random_seeds[n])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "# 文件夹路径\n",
        "grid_folder = r'/content/city2016_lst_ratio_grid_480m_bcr_bhv_ndvi_svf_ev_distbp_distmt_distwb_wr_xy.shp'\n",
        "data = gpd.read_file(grid_folder)\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "axLJStTGfS5L",
        "outputId": "ec2c7d9b-662d-4403-dedb-c753ecce1179",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "axLJStTGfS5L",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               geometry\n",
            "0     POLYGON ((180636.136 449880.582, 180633.582 45...\n",
            "1     POLYGON ((180633.582 450360.745, 180631.028 45...\n",
            "2     POLYGON ((180631.028 450840.909, 180628.474 45...\n",
            "3     POLYGON ((180628.474 451321.073, 180625.919 45...\n",
            "4     POLYGON ((181116.3 449883.135, 181113.746 4503...\n",
            "...                                                 ...\n",
            "1856  POLYGON ((215208.568 450064.457, 215206.014 45...\n",
            "1857  POLYGON ((215206.014 450544.639, 215203.46 451...\n",
            "1858  POLYGON ((215203.46 451024.82, 215200.905 4515...\n",
            "1859  POLYGON ((215200.905 451505.002, 215198.35 451...\n",
            "1860  POLYGON ((215198.35 451985.184, 215195.795 452...\n",
            "\n",
            "[1861 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "295ecb62",
      "metadata": {
        "id": "295ecb62",
        "outputId": "6ec01a55-d87a-4bd6-ceb5-6ef4c698fa78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9394  898 2398 5906 2343 8225 5506 6451 2670 3497 1087 1819 2308 6084\n",
            " 3724 3184 6387 3728 2702 7883]\n",
            "19\n",
            "7883\n"
          ]
        }
      ],
      "source": [
        "# 这个代码干的事情其实就是用梯度提升回归树 (GradientBoostingRegressor, GBDT) 对地理网格数据建模，并做特征重要性分析 + 偏依赖图 (PDP) 提取。\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from scipy.stats import randint, uniform, loguniform\n",
        "import joblib\n",
        "joblib.parallel.BatchCompletionCallBack = lambda *args, **kwargs: None\n",
        "\n",
        "# 文件夹路径\n",
        "grid_folder = r'/content'\n",
        "#####################################################################\n",
        "# 用20个 random seeds\n",
        "np.random.seed(0)  # 固定种子以便复现\n",
        "random_seeds = np.random.choice(10000, size=20, replace=False)\n",
        "print(random_seeds)\n",
        "n = 19\n",
        "print(n)\n",
        "print(random_seeds[n])\n",
        "\n",
        "for year in [2016]:\n",
        "    target_vars = [f'nor_{year}', f'ext_{year}', f'hr_{year}']\n",
        "    explanatory_vars = ['BCR', 'BHV',  'SVF', 'NDVI', 'EV', 'WR', 'Dist_W', 'Dist_P', 'Dist_M','X','Y'] # 顺序很讲究\n",
        "    # 保存结果\n",
        "    all_results = []\n",
        "    r2_comparison = []\n",
        "\n",
        "    param_dist = {\n",
        "        'n_estimators': [4168], #4168\n",
        "        'learning_rate': loguniform(0.002, 0.355), #(0.002, 0.355)\n",
        "        'subsample': uniform(0.545, 0.413), # [0.545,0.958]\n",
        "        'max_depth' : randint(5, 14), # [5, 13]\n",
        "        'min_samples_split':[2], #2\n",
        "        'max_features': uniform(0.335, 0.581), #[0.335,0.916]\n",
        "        }\n",
        "\n",
        "    # === 主循环 ===\n",
        "    for filename in os.listdir(grid_folder):\n",
        "        if filename.endswith(rf'city{year}_lst_ratio_grid_480m_bcr_bhv_ndvi_svf_ev_distbp_distmt_distwb_wr_xy.shp'):\n",
        "            input_path = os.path.join(grid_folder, filename)\n",
        "            match = re.search(r'(\\d{3,5})m', filename)\n",
        "            grid_size = match.group(1)\n",
        "\n",
        "            gdf = gpd.read_file(input_path)\n",
        "            gdf_clean = gdf.replace([np.inf, -np.inf], np.nan).dropna(subset=target_vars + explanatory_vars)\n",
        "\n",
        "            for target in target_vars:\n",
        "                X = gdf_clean[explanatory_vars]\n",
        "                y = gdf_clean[target]\n",
        "\n",
        "                for r in [random_seeds[n]]:\n",
        "                    # 数据划分\n",
        "                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=r) # 切20次\n",
        "                    gbdt = GradientBoostingRegressor(random_state=0)\n",
        "                    cv = RepeatedKFold(n_splits=5, n_repeats=20, random_state=0)\n",
        "\n",
        "                    search = RandomizedSearchCV(\n",
        "                        estimator=gbdt,\n",
        "                        param_distributions=param_dist,\n",
        "                        n_iter= 200,\n",
        "                        scoring='r2',\n",
        "                        cv=cv, # cross validation\n",
        "                        verbose=3,\n",
        "                        n_jobs=1,\n",
        "                        random_state=0\n",
        "                    )\n",
        "\n",
        "                    search.fit(X_train, y_train)\n",
        "                    folder = os.path.join(grid_folder, r'Machine Learning')\n",
        "                    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "                    # 先保存一份 checkpoint (CSV，追加方式)\n",
        "                    checkpoint_path = os.path.join(folder, f\"{year}_{target}_seed{n}_checkpoint.csv\")\n",
        "                    write_header = not os.path.exists(checkpoint_path)\n",
        "                    df_cv = pd.DataFrame(search.cv_results_)\n",
        "                    df_cv.to_csv(checkpoint_path, mode='a', header=write_header, index=False)\n",
        "                    print(f\"[SAVE] checkpoint -> {checkpoint_path}\")\n",
        "\n",
        "\n",
        "                    # 再保存原本的 Excel（完整結果）\n",
        "                    cv_path = os.path.join(folder, f\"{n}_{r}_GBDT_{target}_cv_results.xlsx\")\n",
        "                    try:\n",
        "                        df_cv.to_excel(cv_path, index=False)\n",
        "                        print(f\"[SAVE] CV results -> {cv_path}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] Save CV results failed: {cv_path} | {e}\")\n",
        "                        raise\n",
        "\n",
        "                    # 使用测试集评估\n",
        "                    best_model = search.best_estimator_\n",
        "                    y_train_pred = best_model.predict(X_train)\n",
        "                    y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "                    r2_train = best_model.score(X_train, y_train)\n",
        "                    r2_test = r2_score(y_test, y_test_pred)\n",
        "\n",
        "                    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "                    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "                    print(f\" {filename} | {target} 最佳参数: {search.best_params_} | R²_train={r2_train:.3f} | R²_test={r2_test:.3f}\")\n",
        "\n",
        "                    for var, importance in zip(explanatory_vars, best_model.feature_importances_):\n",
        "                        all_results.append({\n",
        "                            'GridSize': grid_size,\n",
        "                            'Target': target,\n",
        "                            'Feature': var,\n",
        "                            'Random seed':r,\n",
        "                            'FeatureImportance_TrainModel': round(importance, 4),\n",
        "                            'Train_R2': round(r2_train, 4),\n",
        "                            'Train_RMSE': round(rmse_train, 4),\n",
        "                            'Test_R2': round(r2_test, 4),\n",
        "                            'Test_RMSE': round(rmse_test, 4),\n",
        "                            **search.best_params_\n",
        "                        })\n",
        "                        r2_comparison.append({\n",
        "                            'GridSize': grid_size,\n",
        "                            'Target': target,\n",
        "                            'Random seed':r,\n",
        "                            'Train_R2': round(r2_train, 4),\n",
        "                            'Test_R2': round(r2_test, 4),\n",
        "                            'Train_RMSE': round(rmse_train, 4),\n",
        "                            'Test_RMSE': round(rmse_test, 4)\n",
        "                        })\n",
        "\n",
        "            # 保存模型训练后的结果\n",
        "            df_all = pd.DataFrame(all_results)\n",
        "            df_all.to_excel(os.path.join(folder, f'{year} GBDT_Random_Search_Results_{target}_{n}_{r}.xlsx'), index=False)\n",
        "            df_r2 = pd.DataFrame(r2_comparison)\n",
        "            df_r2 = df_r2.sort_values(['Target', 'GridSize'])\n",
        "            df_r2.to_excel(os.path.join(folder, f'{year} R2_Comparison_Train_vs_Test_{target}_{n}_{r}.xlsx'), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FlQIhx9BrbV0"
      },
      "id": "FlQIhx9BrbV0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}